{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize, imshow\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('Project_data/val.csv').readlines())\n",
    "batch_size = 5 #experiment with the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the shape of the train images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(360, 360, 3), (120, 160, 3)}\n"
     ]
    }
   ],
   "source": [
    "source_path='Project_data'\n",
    "train_file=source_path+\"/train.csv\"\n",
    "\n",
    "with open(train_file) as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "# set of image sizes\n",
    "s = set()\n",
    "\n",
    "# For all images in the training set, find the shape and add it to the set. The set will hence contain all the different image sizes.\n",
    "for l in lines:\n",
    "    folder=source_path + \"/train\"+\"/\" + l.split(';')[0]\n",
    "    imgs = os.listdir(folder)\n",
    "    for i in imgs:\n",
    "        img_name=folder + \"/\" + i\n",
    "        image = imread(img_name)\n",
    "        s.add(image.shape)\n",
    "\n",
    "print(s)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are two image sizes in the training data. To make it uniform we will resize all the images to size (120, 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets experiment resize, normalize and show apis on one single image before we use them in the generator. We can see how these functions modify the original entities. We will first try an image of size 360X360. In this case we will just resize it to size 120X120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 360, 3)\n",
      "(120, 120, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46666667 0.47058824 0.4627451  ... 0.54509804 0.5372549  0.52941176]\n",
      " [0.47843137 0.4745098  0.47058824 ... 0.5372549  0.52941176 0.5254902 ]\n",
      " [0.51372549 0.50980392 0.50980392 ... 0.53333333 0.5254902  0.52156863]\n",
      " ...\n",
      " [0.46666667 0.32941176 0.27058824 ... 0.25098039 0.27843137 0.29019608]\n",
      " [0.29019608 0.16078431 0.19607843 ... 0.25490196 0.29019608 0.29411765]\n",
      " [0.14117647 0.08627451 0.1254902  ... 0.2627451  0.28627451 0.28627451]]\n"
     ]
    }
   ],
   "source": [
    "img_name = 'Project_data/train/WIN_20180907_15_35_09_Pro_Right Swipe_new/WIN_20180907_15_35_09_Pro_00012.png'\n",
    "image = imread(img_name)\n",
    "# The original size of the image\n",
    "print(image.shape)\n",
    "resized = imresize(image, (120,120))\n",
    "# size of the image after resize\n",
    "print(resized.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(resized[:, : , 0])\n",
    "plt.show()\n",
    "print(resized[:, : , 0]/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will experiment on an image of size 120X160. In this case we will crop it to size 120X120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 160, 3)\n",
      "(120, 120, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.64313725 0.76078431 0.9372549  ... 0.39215686 0.36862745 0.36078431]\n",
      " [0.65098039 0.63921569 0.6627451  ... 0.38823529 0.36862745 0.35686275]\n",
      " [0.61568627 0.59607843 0.58431373 ... 0.38823529 0.36862745 0.35686275]\n",
      " ...\n",
      " [1.         1.         1.         ... 0.36862745 0.34117647 0.3254902 ]\n",
      " [0.45490196 0.89803922 1.         ... 0.35686275 0.36078431 0.34117647]\n",
      " [1.         0.98431373 0.97254902 ... 0.36862745 0.37254902 0.36470588]]\n"
     ]
    }
   ],
   "source": [
    "img_name = 'Project_data/train/WIN_20180925_17_08_43_Pro_Left_Swipe_new/WIN_20180925_17_08_43_Pro_00016.png'\n",
    "image = imread(img_name)\n",
    "# The original size of the image\n",
    "print(image.shape)\n",
    "resized = image[0:120,0:120]\n",
    "# size of the image after resize\n",
    "print(resized.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(resized[:, : , 0])\n",
    "plt.show()\n",
    "print(resized[:, : , 0]/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    \n",
    "    # We have experimented with different img_idx. To ensure fast training we sampled 20 images (leaving out multiples of 3).\n",
    "    # Once we finalize a good model we took all 30 images and ran training for 40 epochs.\n",
    "    \n",
    "    #img_idx = [i for i in range(0, 30) if i % 3 != 0]\n",
    "    img_idx = [i for i in range(0, 30)]\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(t)/batch_size) # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,30,120,160,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item  in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    #resized = imresize(image, (120,160))\n",
    "                    \n",
    "                    if image.shape[0] == 360:\n",
    "                      # in case of 360 X 360 , resize the image to 120X120\n",
    "                      resized = imresize(image, (120,120))\n",
    "                    else:\n",
    "                      # in case of 120 X 160 , crop the image to 120X120\n",
    "                      resized = image[0:120,0:120]\n",
    "                      \n",
    "                    batch_data[folder,idx,:,:,0] = resized[:, : , 0]/255\n",
    "                    batch_data[folder,idx,:,:,1] = resized[:, : , 1]/255\n",
    "                    batch_data[folder,idx,:,:,2] = resized[:, : , 2]/255\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        remaining = len(t) - num_batches*batch_size\n",
    "        if remaining > 0:\n",
    "            batch_data = np.zeros((remaining,30,120,160,3))\n",
    "            batch_labels = np.zeros((remaining,5))\n",
    "            for folder in range(remaining):\n",
    "                imgs = os.listdir(source_path+\"/\"+t[num_batches*batch_size + folder].split(';')[0])\n",
    "                for idx,item in enumerate(img_idx):\n",
    "                #for i in imgs:\n",
    "                    image = imread(source_path+\"/\"+t[num_batches*batch_size + folder].split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #resized = imresize(image, (120,160))\n",
    "                    \n",
    "                    if image.shape[0] == 360:\n",
    "                      # in case of 360 X 360 , resize the image to 120X120\n",
    "                      resized = imresize(image, (120,120))\n",
    "                    else:\n",
    "                      # in case of 120 X 160 , crop the image to 120X120\n",
    "                      resized = image[0:120,0:120]\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = resized[:, : , 0]/255\n",
    "                    batch_data[folder,idx,:,:,1] = resized[:, : , 1]/255\n",
    "                    batch_data[folder,idx,:,:,2] = resized[:, : , 2]/255\n",
    "\n",
    "                batch_labels[folder, int(t[folder + num_batches*batch_size].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 40\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 40 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have experimented with different value of \"num_epochs\" like 10, 15, 20 and 40."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_15 (Conv3D)           (None, 18, 118, 158, 16)  1312      \n",
      "_________________________________________________________________\n",
      "conv3d_16 (Conv3D)           (None, 16, 116, 156, 16)  6928      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling (None, 8, 58, 78, 16)     0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 8, 58, 78, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_17 (Conv3D)           (None, 6, 56, 76, 32)     13856     \n",
      "_________________________________________________________________\n",
      "conv3d_18 (Conv3D)           (None, 4, 54, 74, 32)     27680     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling (None, 2, 27, 37, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 2, 27, 37, 32)     0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 63936)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               8183936   \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 8,234,357\n",
      "Trainable params: 8,234,357\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#write your model here\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Conv3D(\n",
    "    16, (3,3,3), activation='relu', input_shape=(20,120,160,3)\n",
    "))\n",
    "model.add(Conv3D(16, (3,3,3), activation='relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(32, (3,3,3), activation='relu'))\n",
    "model.add(Conv3D(32, (3,3,3), activation='relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "optimiser = Adam() #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "model_name = 'model_1' + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "#filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "filepath = model_name + 'model.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_per_epoch 133\n",
      "validation_steps 20\n"
     ]
    }
   ],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "print ('steps_per_epoch %d' % steps_per_epoch)\n",
    "print ('validation_steps %d' % validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  del sys.path[0]\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/133 [==========================>...] - ETA: 4s - loss: 1.6595 - categorical_accuracy: 0.1967"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 65s 492ms/step - loss: 1.6559 - categorical_accuracy: 0.1980 - val_loss: 1.6093 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00001: saving model to model_1/model.h5\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 59s 445ms/step - loss: 1.6096 - categorical_accuracy: 0.1790 - val_loss: 1.6087 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00002: saving model to model_1/model.h5\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 60s 448ms/step - loss: 1.6099 - categorical_accuracy: 0.1865 - val_loss: 1.6085 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00003: saving model to model_1/model.h5\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 59s 441ms/step - loss: 1.6090 - categorical_accuracy: 0.1865 - val_loss: 1.6084 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00004: saving model to model_1/model.h5\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 59s 444ms/step - loss: 1.6091 - categorical_accuracy: 0.2030 - val_loss: 1.6079 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00005: saving model to model_1/model.h5\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 59s 443ms/step - loss: 1.6093 - categorical_accuracy: 0.2070 - val_loss: 1.6077 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00006: saving model to model_1/model.h5\n",
      "Epoch 7/10\n",
      "133/133 [==============================] - 60s 448ms/step - loss: 1.6090 - categorical_accuracy: 0.2020 - val_loss: 1.6075 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00007: saving model to model_1/model.h5\n",
      "Epoch 8/10\n",
      "133/133 [==============================] - 59s 447ms/step - loss: 1.6089 - categorical_accuracy: 0.2200 - val_loss: 1.6073 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00008: saving model to model_1/model.h5\n",
      "Epoch 9/10\n",
      "133/133 [==============================] - 59s 446ms/step - loss: 1.6094 - categorical_accuracy: 0.2060 - val_loss: 1.6071 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00009: saving model to model_1/model.h5\n",
      "Epoch 10/10\n",
      "133/133 [==============================] - 59s 445ms/step - loss: 1.6093 - categorical_accuracy: 0.2015 - val_loss: 1.6072 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00010: saving model to model_1/model.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff827d4b278>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, we can conclude that the model 1 is not able to learn much from the training data. We need to modify the architecture to see how that influences results. We will go with simpler models compared to model 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 20, 120, 160, 8)   656       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 10, 60, 80, 8)     0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 10, 60, 80, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 10, 60, 80, 16)    3472      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 5, 30, 40, 16)     0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 5, 30, 40, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 5, 30, 40, 32)     13856     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 2, 15, 20, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2, 15, 20, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 2, 15, 20, 64)     55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 1, 7, 10, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1, 7, 10, 64)      0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4480)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               1147136   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 1,221,765\n",
      "Trainable params: 1,221,765\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(\n",
    "    8, (3,3,3), activation='relu', input_shape=(20,120,160,3),\n",
    "    padding='same'\n",
    "))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(16, (3,3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(32, (3,3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(64, (3,3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "optimiser = Adam()\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  del sys.path[0]\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/133 [===========================>..] - ETA: 3s - loss: 1.6090 - categorical_accuracy: 0.1904"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 62s 463ms/step - loss: 1.6089 - categorical_accuracy: 0.1940 - val_loss: 1.6072 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.60716, saving model to model_2/model.h5\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 60s 450ms/step - loss: 1.6089 - categorical_accuracy: 0.1875 - val_loss: 1.6070 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.60716 to 1.60702, saving model to model_2/model.h5\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 59s 445ms/step - loss: 1.6091 - categorical_accuracy: 0.1965 - val_loss: 1.6070 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.60702\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 59s 444ms/step - loss: 1.6092 - categorical_accuracy: 0.1739 - val_loss: 1.6071 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.60702\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 59s 443ms/step - loss: 1.6087 - categorical_accuracy: 0.1820 - val_loss: 1.6070 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.60702 to 1.60698, saving model to model_2/model.h5\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 60s 448ms/step - loss: 1.6091 - categorical_accuracy: 0.1805 - val_loss: 1.6070 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.60698\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 7/10\n",
      "133/133 [==============================] - 59s 441ms/step - loss: 1.6087 - categorical_accuracy: 0.2130 - val_loss: 1.6070 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.60698\n",
      "Epoch 8/10\n",
      "133/133 [==============================] - 59s 443ms/step - loss: 1.6087 - categorical_accuracy: 0.1880 - val_loss: 1.6071 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.60698\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 9/10\n",
      "133/133 [==============================] - 59s 441ms/step - loss: 1.6089 - categorical_accuracy: 0.2045 - val_loss: 1.6070 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.60698\n",
      "Epoch 10/10\n",
      "133/133 [==============================] - 59s 444ms/step - loss: 1.6086 - categorical_accuracy: 0.1875 - val_loss: 1.6071 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.60698\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff82c02a2e8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_2' + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "#filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "filepath = model_name + 'model.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is still some problem with the CNN. We expected the loss to reduce in every epoch, but its simply stuck at 1.6086. Lets choose a simpler network with the CNN layers arranged a little differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_42 (Conv3D)           (None, 20, 120, 160, 8)   656       \n",
      "_________________________________________________________________\n",
      "conv3d_43 (Conv3D)           (None, 20, 120, 160, 8)   1736      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_25 (MaxPooling (None, 10, 60, 80, 8)     0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 10, 60, 80, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_44 (Conv3D)           (None, 10, 60, 80, 16)    3472      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_26 (MaxPooling (None, 5, 30, 40, 16)     0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 5, 30, 40, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_45 (Conv3D)           (None, 5, 30, 40, 32)     13856     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_27 (MaxPooling (None, 2, 15, 20, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 2, 15, 20, 32)     0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 19200)             0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 128)               2457728   \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 2,478,093\n",
      "Trainable params: 2,478,093\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(\n",
    "    8, (3,3,3), activation='relu', input_shape=(20,120,160,3),\n",
    "    padding='same'\n",
    "))\n",
    "model.add(Conv3D(8, (3,3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(16, (3,3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(32, (3,3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "optimiser = Adam()\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  del sys.path[0]\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 1.7041 - categorical_accuracy: 0.1802"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 57s 429ms/step - loss: 1.7026 - categorical_accuracy: 0.1805 - val_loss: 1.6092 - val_categorical_accuracy: 0.1800\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.60925, saving model to model_3/model.h5\n",
      "Epoch 2/15\n",
      "133/133 [==============================] - 53s 399ms/step - loss: 1.6095 - categorical_accuracy: 0.2000 - val_loss: 1.6025 - val_categorical_accuracy: 0.2100\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.60925 to 1.60253, saving model to model_3/model.h5\n",
      "Epoch 3/15\n",
      "133/133 [==============================] - 54s 403ms/step - loss: 1.6045 - categorical_accuracy: 0.2000 - val_loss: 1.5638 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.60253 to 1.56376, saving model to model_3/model.h5\n",
      "Epoch 4/15\n",
      "133/133 [==============================] - 54s 402ms/step - loss: 1.3705 - categorical_accuracy: 0.3815 - val_loss: 1.1140 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.56376 to 1.11404, saving model to model_3/model.h5\n",
      "Epoch 5/15\n",
      "133/133 [==============================] - 54s 407ms/step - loss: 1.1324 - categorical_accuracy: 0.4982 - val_loss: 0.9908 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.11404 to 0.99085, saving model to model_3/model.h5\n",
      "Epoch 6/15\n",
      "133/133 [==============================] - 53s 401ms/step - loss: 0.9786 - categorical_accuracy: 0.5534 - val_loss: 0.9247 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.99085 to 0.92469, saving model to model_3/model.h5\n",
      "Epoch 7/15\n",
      "133/133 [==============================] - 54s 408ms/step - loss: 0.8683 - categorical_accuracy: 0.6426 - val_loss: 0.9797 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.92469\n",
      "Epoch 8/15\n",
      "133/133 [==============================] - 53s 401ms/step - loss: 0.6971 - categorical_accuracy: 0.7168 - val_loss: 0.7081 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.92469 to 0.70805, saving model to model_3/model.h5\n",
      "Epoch 9/15\n",
      "133/133 [==============================] - 54s 409ms/step - loss: 0.5535 - categorical_accuracy: 0.7865 - val_loss: 0.7537 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.70805\n",
      "Epoch 10/15\n",
      "133/133 [==============================] - 53s 401ms/step - loss: 0.2974 - categorical_accuracy: 0.8767 - val_loss: 0.9150 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.70805\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 11/15\n",
      "133/133 [==============================] - 53s 398ms/step - loss: 0.1820 - categorical_accuracy: 0.9398 - val_loss: 0.8342 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.70805\n",
      "Epoch 12/15\n",
      "133/133 [==============================] - 54s 405ms/step - loss: 0.1352 - categorical_accuracy: 0.9494 - val_loss: 0.8657 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.70805\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 13/15\n",
      "133/133 [==============================] - 54s 403ms/step - loss: 0.0694 - categorical_accuracy: 0.9805 - val_loss: 0.9966 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.70805\n",
      "Epoch 14/15\n",
      "133/133 [==============================] - 53s 400ms/step - loss: 0.0526 - categorical_accuracy: 0.9850 - val_loss: 1.0295 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.70805\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 15/15\n",
      "133/133 [==============================] - 54s 408ms/step - loss: 0.0418 - categorical_accuracy: 0.9880 - val_loss: 1.0537 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.70805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4b3bdb5668>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_3' + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "#filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "filepath = model_name + 'model.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are now looking much better. We have hit a validation accuracy of 76%. Next, our aim is to optimize the network even further and reach over 80% validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_49 (Conv3D)           (None, 20, 120, 160, 8)   656       \n",
      "_________________________________________________________________\n",
      "conv3d_50 (Conv3D)           (None, 20, 120, 160, 16)  3472      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_30 (MaxPooling (None, 10, 60, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 10, 60, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_51 (Conv3D)           (None, 10, 60, 80, 32)    13856     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_31 (MaxPooling (None, 5, 30, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 5, 30, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 192000)            0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                12288064  \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 12,306,373\n",
      "Trainable params: 12,306,373\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(\n",
    "    8, (3,3,3), activation='relu', input_shape=(20,120,160,3),\n",
    "    padding='same'\n",
    "))\n",
    "\n",
    "model.add(Conv3D(16, (3,3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv3D(32, (3,3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "optimiser = Adam()\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  del sys.path[0]\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 2.3391 - categorical_accuracy: 0.2015"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 59s 446ms/step - loss: 2.3281 - categorical_accuracy: 0.2050 - val_loss: 1.6089 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.60887, saving model to model_3/model.h5\n",
      "Epoch 2/15\n",
      "133/133 [==============================] - 57s 427ms/step - loss: 1.5993 - categorical_accuracy: 0.1925 - val_loss: 1.6091 - val_categorical_accuracy: 0.2200\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.60887\n",
      "Epoch 3/15\n",
      "133/133 [==============================] - 56s 421ms/step - loss: 1.6357 - categorical_accuracy: 0.2301 - val_loss: 1.6059 - val_categorical_accuracy: 0.2000\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.60887 to 1.60592, saving model to model_3/model.h5\n",
      "Epoch 4/15\n",
      "133/133 [==============================] - 57s 427ms/step - loss: 1.5386 - categorical_accuracy: 0.2752 - val_loss: 1.4296 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.60592 to 1.42962, saving model to model_3/model.h5\n",
      "Epoch 5/15\n",
      "133/133 [==============================] - 56s 423ms/step - loss: 1.4281 - categorical_accuracy: 0.4040 - val_loss: 1.4454 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.42962\n",
      "Epoch 6/15\n",
      "133/133 [==============================] - 57s 429ms/step - loss: 1.2762 - categorical_accuracy: 0.4822 - val_loss: 1.3527 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.42962 to 1.35269, saving model to model_3/model.h5\n",
      "Epoch 7/15\n",
      "133/133 [==============================] - 56s 420ms/step - loss: 1.0599 - categorical_accuracy: 0.6030 - val_loss: 1.4011 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.35269\n",
      "Epoch 8/15\n",
      "133/133 [==============================] - 57s 425ms/step - loss: 0.8795 - categorical_accuracy: 0.6702 - val_loss: 1.3558 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.35269\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 9/15\n",
      "133/133 [==============================] - 56s 424ms/step - loss: 0.6319 - categorical_accuracy: 0.7819 - val_loss: 1.2840 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.35269 to 1.28397, saving model to model_3/model.h5\n",
      "Epoch 10/15\n",
      "133/133 [==============================] - 56s 424ms/step - loss: 0.5339 - categorical_accuracy: 0.8125 - val_loss: 1.3593 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.28397\n",
      "Epoch 11/15\n",
      "133/133 [==============================] - 56s 417ms/step - loss: 0.4120 - categorical_accuracy: 0.8571 - val_loss: 1.2436 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.28397 to 1.24356, saving model to model_3/model.h5\n",
      "Epoch 12/15\n",
      "133/133 [==============================] - 56s 417ms/step - loss: 0.3453 - categorical_accuracy: 0.8917 - val_loss: 1.4142 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.24356\n",
      "Epoch 13/15\n",
      "133/133 [==============================] - 56s 422ms/step - loss: 0.2563 - categorical_accuracy: 0.9158 - val_loss: 1.5899 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.24356\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 14/15\n",
      "133/133 [==============================] - 57s 427ms/step - loss: 0.2390 - categorical_accuracy: 0.9233 - val_loss: 1.6078 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.24356\n",
      "Epoch 15/15\n",
      "133/133 [==============================] - 56s 420ms/step - loss: 0.2158 - categorical_accuracy: 0.9203 - val_loss: 1.6550 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.24356\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4b3b1487b8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_4' + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "#filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "filepath = model_name + 'model.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are not able to achieve our target accuracy, we will now add some BatchNormalization layers to see if that normalization of the input to the activation function can help us improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 20, 120, 160, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 20, 120, 160, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20, 120, 160, 8)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 20, 120, 160, 8)   1736      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 20, 120, 160, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 20, 120, 160, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 10, 60, 80, 8)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 60, 80, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 10, 60, 80, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 10, 60, 80, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10, 60, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 10, 60, 80, 32)    13856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 10, 60, 80, 32)    128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10, 60, 80, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 5, 30, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 30, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 192000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               49152256  \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 49,174,541\n",
      "Trainable params: 49,173,901\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(\n",
    "    8, (3,3,3), input_shape=(20,120,160,3),\n",
    "    padding='same'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(8, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(16, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv3D(32, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "optimiser = Adam()\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  del sys.path[0]\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 2s - loss: 1.4661 - categorical_accuracy: 0.4489"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 197s 1s/step - loss: 1.4538 - categorical_accuracy: 0.4556 - val_loss: 1.2187 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.21867, saving model to model_5/model.h5\n",
      "Epoch 2/15\n",
      "133/133 [==============================] - 63s 472ms/step - loss: 0.9135 - categorical_accuracy: 0.6481 - val_loss: 1.4100 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.21867\n",
      "Epoch 3/15\n",
      "133/133 [==============================] - 63s 473ms/step - loss: 0.7161 - categorical_accuracy: 0.7283 - val_loss: 0.8620 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.21867 to 0.86203, saving model to model_5/model.h5\n",
      "Epoch 4/15\n",
      "133/133 [==============================] - 63s 475ms/step - loss: 0.6280 - categorical_accuracy: 0.7599 - val_loss: 0.8375 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.86203 to 0.83752, saving model to model_5/model.h5\n",
      "Epoch 5/15\n",
      "133/133 [==============================] - 63s 474ms/step - loss: 0.4752 - categorical_accuracy: 0.8296 - val_loss: 2.8858 - val_categorical_accuracy: 0.2900\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.83752\n",
      "Epoch 6/15\n",
      "133/133 [==============================] - 63s 476ms/step - loss: 0.4009 - categorical_accuracy: 0.8576 - val_loss: 1.0208 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.83752\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 7/15\n",
      "133/133 [==============================] - 63s 471ms/step - loss: 0.2448 - categorical_accuracy: 0.9193 - val_loss: 0.9640 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.83752\n",
      "Epoch 8/15\n",
      "133/133 [==============================] - 63s 471ms/step - loss: 0.1666 - categorical_accuracy: 0.9519 - val_loss: 1.2523 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.83752\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 9/15\n",
      "133/133 [==============================] - 63s 472ms/step - loss: 0.1643 - categorical_accuracy: 0.9579 - val_loss: 0.9431 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.83752\n",
      "Epoch 10/15\n",
      "133/133 [==============================] - 63s 473ms/step - loss: 0.1414 - categorical_accuracy: 0.9564 - val_loss: 0.7676 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.83752 to 0.76756, saving model to model_5/model.h5\n",
      "Epoch 11/15\n",
      "133/133 [==============================] - 63s 475ms/step - loss: 0.1488 - categorical_accuracy: 0.9534 - val_loss: 1.0081 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.76756\n",
      "Epoch 12/15\n",
      "133/133 [==============================] - 63s 475ms/step - loss: 0.1247 - categorical_accuracy: 0.9744 - val_loss: 1.0372 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.76756\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/15\n",
      "133/133 [==============================] - 63s 472ms/step - loss: 0.0775 - categorical_accuracy: 0.9895 - val_loss: 0.9897 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.76756\n",
      "Epoch 14/15\n",
      "133/133 [==============================] - 63s 473ms/step - loss: 0.0916 - categorical_accuracy: 0.9850 - val_loss: 0.9619 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.76756\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 15/15\n",
      "133/133 [==============================] - 63s 474ms/step - loss: 0.0764 - categorical_accuracy: 0.9789 - val_loss: 0.8081 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.76756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe1fc75e080>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_5' + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "#filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "filepath = model_name + 'model.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_5 (Conv3D)            (None, 20, 120, 160, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 20, 120, 160, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 20, 120, 160, 8)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 20, 120, 160, 8)   1736      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 20, 120, 160, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 20, 120, 160, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 10, 60, 80, 8)     0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 10, 60, 80, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 10, 60, 80, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 10, 60, 80, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10, 60, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_8 (Conv3D)            (None, 10, 60, 80, 16)    6928      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 10, 60, 80, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10, 60, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 5, 30, 40, 16)     0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 5, 30, 40, 16)     0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 96000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               12288128  \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 12,302,269\n",
      "Trainable params: 12,301,917\n",
      "Non-trainable params: 352\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(\n",
    "    8, (3,3,3), input_shape=(20,120,160,3),\n",
    "    padding='same'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(8, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv3D(16, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv3D(16, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "optimiser = Adam()\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  del sys.path[0]\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 1s - loss: 1.7015 - categorical_accuracy: 0.3814"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 58s 434ms/step - loss: 1.7009 - categorical_accuracy: 0.3824 - val_loss: 2.4749 - val_categorical_accuracy: 0.2400\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.24000, saving model to model_5/model.h5\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 54s 409ms/step - loss: 1.3902 - categorical_accuracy: 0.4747 - val_loss: 2.2641 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.24000 to 0.25000, saving model to model_5/model.h5\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 54s 408ms/step - loss: 1.1592 - categorical_accuracy: 0.5489 - val_loss: 2.0507 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.25000 to 0.34000, saving model to model_5/model.h5\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 54s 407ms/step - loss: 1.0183 - categorical_accuracy: 0.5805 - val_loss: 1.2268 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.34000 to 0.46000, saving model to model_5/model.h5\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 54s 406ms/step - loss: 0.9535 - categorical_accuracy: 0.6246 - val_loss: 1.2090 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.46000 to 0.52000, saving model to model_5/model.h5\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 55s 412ms/step - loss: 0.8576 - categorical_accuracy: 0.6582 - val_loss: 1.3217 - val_categorical_accuracy: 0.4200\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.52000\n",
      "Epoch 7/10\n",
      "133/133 [==============================] - 54s 405ms/step - loss: 0.7056 - categorical_accuracy: 0.7489 - val_loss: 1.0150 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.52000 to 0.62000, saving model to model_5/model.h5\n",
      "Epoch 8/10\n",
      "133/133 [==============================] - 54s 407ms/step - loss: 0.7076 - categorical_accuracy: 0.7268 - val_loss: 1.2351 - val_categorical_accuracy: 0.4900\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.62000\n",
      "Epoch 9/10\n",
      "133/133 [==============================] - 54s 407ms/step - loss: 0.6377 - categorical_accuracy: 0.7479 - val_loss: 1.3016 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.62000\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 10/10\n",
      "133/133 [==============================] - 57s 429ms/step - loss: 0.5059 - categorical_accuracy: 0.8165 - val_loss: 0.8416 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.62000 to 0.67000, saving model to model_5/model.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe1c411fc50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_6' + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "#filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "filepath = model_name + 'model.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_13 (Conv3D)           (None, 20, 120, 160, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 20, 120, 160, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 20, 120, 160, 8)   0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 20, 120, 160, 8)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_14 (Conv3D)           (None, 20, 120, 160, 16)  3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 20, 120, 160, 16)  64        \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 20, 120, 160, 16)  0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 20, 120, 160, 16)  0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_12 (MaxPooling (None, 10, 60, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_15 (Conv3D)           (None, 10, 60, 80, 32)    13856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 10, 60, 80, 32)    128       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10, 60, 80, 32)    0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 10, 60, 80, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_13 (MaxPooling (None, 5, 30, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 5, 30, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 192000)            0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 256)               49152256  \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 49,205,541\n",
      "Trainable params: 49,204,661\n",
      "Non-trainable params: 880\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(\n",
    "    8, (3,3,3), input_shape=(20,120,160,3),\n",
    "    padding='same'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(16, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(32, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "optimiser = Adam()\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  del sys.path[0]\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/133 [==========================>...] - ETA: 5s - loss: 1.7421 - categorical_accuracy: 0.3082"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:35: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 73s 548ms/step - loss: 1.7171 - categorical_accuracy: 0.3123 - val_loss: 2.8447 - val_categorical_accuracy: 0.1600\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.16000, saving model to model_7/model.h5\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 66s 497ms/step - loss: 1.5344 - categorical_accuracy: 0.3865 - val_loss: 2.7364 - val_categorical_accuracy: 0.1900\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.16000 to 0.19000, saving model to model_7/model.h5\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 66s 496ms/step - loss: 1.2642 - categorical_accuracy: 0.4887 - val_loss: 3.6635 - val_categorical_accuracy: 0.1700\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.19000\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 67s 500ms/step - loss: 1.1895 - categorical_accuracy: 0.5243 - val_loss: 3.5406 - val_categorical_accuracy: 0.3200\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.19000 to 0.32000, saving model to model_7/model.h5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 69s 519ms/step - loss: 0.9987 - categorical_accuracy: 0.6035 - val_loss: 2.7134 - val_categorical_accuracy: 0.3400\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.32000 to 0.34000, saving model to model_7/model.h5\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 69s 516ms/step - loss: 0.9440 - categorical_accuracy: 0.6346 - val_loss: 3.4461 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.34000\n",
      "Epoch 7/10\n",
      "133/133 [==============================] - 68s 513ms/step - loss: 0.9132 - categorical_accuracy: 0.6607 - val_loss: 3.0648 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.34000\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 8/10\n",
      "133/133 [==============================] - 69s 515ms/step - loss: 0.7819 - categorical_accuracy: 0.7098 - val_loss: 3.4970 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.34000\n",
      "Epoch 9/10\n",
      "133/133 [==============================] - 68s 515ms/step - loss: 0.7232 - categorical_accuracy: 0.7343 - val_loss: 3.6926 - val_categorical_accuracy: 0.2700\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.34000\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/10\n",
      "133/133 [==============================] - 69s 517ms/step - loss: 0.6720 - categorical_accuracy: 0.7454 - val_loss: 3.1579 - val_categorical_accuracy: 0.3300\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.34000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f50371bb550>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_7' + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "#filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "filepath = model_name + 'model.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_36 (Conv3D)           (None, 30, 120, 160, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 30, 120, 160, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 30, 120, 160, 8)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_37 (Conv3D)           (None, 30, 120, 160, 8)   1736      \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 30, 120, 160, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 30, 120, 160, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_26 (MaxPooling (None, 15, 60, 80, 8)     0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 15, 60, 80, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_38 (Conv3D)           (None, 15, 60, 80, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 15, 60, 80, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 15, 60, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_39 (Conv3D)           (None, 15, 60, 80, 16)    6928      \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 15, 60, 80, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 15, 60, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_27 (MaxPooling (None, 7, 30, 40, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_40 (Conv3D)           (None, 7, 30, 40, 32)     13856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 7, 30, 40, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 7, 30, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_28 (MaxPooling (None, 3, 15, 20, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 3, 15, 20, 32)     0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 28800)             0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 256)               7373056   \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 7,435,101\n",
      "Trainable params: 7,434,173\n",
      "Non-trainable params: 928\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(\n",
    "    8, (3,3,3), input_shape=(30,120,160,3),\n",
    "    padding='same'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(8, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(16, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv3D(16, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(32, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "optimiser = Adam()\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "123/133 [==========================>...] - ETA: 6s - loss: 1.5319 - categorical_accuracy: 0.3659"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 94s 706ms/step - loss: 1.5286 - categorical_accuracy: 0.3679 - val_loss: 1.2355 - val_categorical_accuracy: 0.5200\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.52000, saving model to model_8/model.h5\n",
      "Epoch 2/20\n",
      "133/133 [==============================] - 88s 664ms/step - loss: 1.2731 - categorical_accuracy: 0.4942 - val_loss: 2.0901 - val_categorical_accuracy: 0.3100\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy did not improve from 0.52000\n",
      "Epoch 3/20\n",
      "133/133 [==============================] - 88s 665ms/step - loss: 1.1474 - categorical_accuracy: 0.5378 - val_loss: 1.7218 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.52000\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 4/20\n",
      "133/133 [==============================] - 89s 667ms/step - loss: 0.9362 - categorical_accuracy: 0.6541 - val_loss: 1.1677 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.52000 to 0.57000, saving model to model_8/model.h5\n",
      "Epoch 5/20\n",
      "133/133 [==============================] - 89s 672ms/step - loss: 0.8669 - categorical_accuracy: 0.6591 - val_loss: 0.9532 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.57000 to 0.64000, saving model to model_8/model.h5\n",
      "Epoch 6/20\n",
      "133/133 [==============================] - 88s 662ms/step - loss: 0.7945 - categorical_accuracy: 0.6952 - val_loss: 1.0940 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.64000\n",
      "Epoch 7/20\n",
      "133/133 [==============================] - 89s 671ms/step - loss: 0.6842 - categorical_accuracy: 0.7439 - val_loss: 1.0408 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.64000\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 8/20\n",
      "133/133 [==============================] - 88s 665ms/step - loss: 0.5554 - categorical_accuracy: 0.8015 - val_loss: 0.8273 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy improved from 0.64000 to 0.68000, saving model to model_8/model.h5\n",
      "Epoch 9/20\n",
      "133/133 [==============================] - 89s 668ms/step - loss: 0.5880 - categorical_accuracy: 0.7885 - val_loss: 1.0568 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.68000\n",
      "Epoch 10/20\n",
      "133/133 [==============================] - 88s 665ms/step - loss: 0.4591 - categorical_accuracy: 0.8436 - val_loss: 1.0344 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.68000\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 11/20\n",
      "133/133 [==============================] - 88s 664ms/step - loss: 0.5271 - categorical_accuracy: 0.8201 - val_loss: 0.8711 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.68000\n",
      "Epoch 12/20\n",
      "133/133 [==============================] - 89s 667ms/step - loss: 0.4536 - categorical_accuracy: 0.8486 - val_loss: 1.0043 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.68000\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 13/20\n",
      "133/133 [==============================] - 89s 668ms/step - loss: 0.3765 - categorical_accuracy: 0.8797 - val_loss: 0.8046 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.68000\n",
      "Epoch 14/20\n",
      "133/133 [==============================] - 89s 666ms/step - loss: 0.4038 - categorical_accuracy: 0.8697 - val_loss: 0.7976 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00014: val_categorical_accuracy improved from 0.68000 to 0.73000, saving model to model_8/model.h5\n",
      "Epoch 15/20\n",
      "133/133 [==============================] - 89s 666ms/step - loss: 0.3873 - categorical_accuracy: 0.8872 - val_loss: 0.8551 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.73000\n",
      "Epoch 16/20\n",
      "133/133 [==============================] - 89s 668ms/step - loss: 0.3387 - categorical_accuracy: 0.8842 - val_loss: 0.8097 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00016: val_categorical_accuracy improved from 0.73000 to 0.73000, saving model to model_8/model.h5\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 17/20\n",
      "133/133 [==============================] - 89s 671ms/step - loss: 0.3306 - categorical_accuracy: 0.9038 - val_loss: 0.8199 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.73000\n",
      "Epoch 18/20\n",
      "133/133 [==============================] - 89s 669ms/step - loss: 0.3625 - categorical_accuracy: 0.8922 - val_loss: 0.8230 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.73000\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 19/20\n",
      "133/133 [==============================] - 89s 667ms/step - loss: 0.3347 - categorical_accuracy: 0.8872 - val_loss: 0.8159 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.73000\n",
      "Epoch 20/20\n",
      "133/133 [==============================] - 89s 667ms/step - loss: 0.3179 - categorical_accuracy: 0.9083 - val_loss: 0.8450 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.73000\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f50311952b0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_8' + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "#filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "filepath = model_name + 'model.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per our analysis of the different models, Model 8 looks quite good. We can see that at different epochs it has a stable accuracy of nearly 70% on the validation set. If we increase the number of epochs from 20 to 40, probably Model 8 can lead us to over 80% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_46 (Conv3D)           (None, 30, 120, 160, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 30, 120, 160, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 30, 120, 160, 8)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_47 (Conv3D)           (None, 30, 120, 160, 8)   1736      \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 30, 120, 160, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 30, 120, 160, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_32 (MaxPooling (None, 15, 60, 80, 8)     0         \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 15, 60, 80, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_48 (Conv3D)           (None, 15, 60, 80, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_59 (Batc (None, 15, 60, 80, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 15, 60, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_49 (Conv3D)           (None, 15, 60, 80, 16)    6928      \n",
      "_________________________________________________________________\n",
      "batch_normalization_60 (Batc (None, 15, 60, 80, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 15, 60, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_33 (MaxPooling (None, 7, 30, 40, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_50 (Conv3D)           (None, 7, 30, 40, 32)     13856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_61 (Batc (None, 7, 30, 40, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 7, 30, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_34 (MaxPooling (None, 3, 15, 20, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 3, 15, 20, 32)     0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 28800)             0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 256)               7373056   \n",
      "_________________________________________________________________\n",
      "batch_normalization_62 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_63 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 7,435,101\n",
      "Trainable params: 7,434,173\n",
      "Non-trainable params: 928\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(\n",
    "    8, (3,3,3), input_shape=(30,120,160,3),\n",
    "    padding='same'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(8, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(16, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv3D(16, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(32, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "optimiser = Adam()\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/133 [==========================>...] - ETA: 7s - loss: 1.5428 - categorical_accuracy: 0.3475"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 96s 720ms/step - loss: 1.5458 - categorical_accuracy: 0.3514 - val_loss: 2.4647 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.25000, saving model to model_9/model.h5\n",
      "Epoch 2/40\n",
      "133/133 [==============================] - 89s 671ms/step - loss: 1.3182 - categorical_accuracy: 0.4491 - val_loss: 1.4263 - val_categorical_accuracy: 0.4700\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.25000 to 0.47000, saving model to model_9/model.h5\n",
      "Epoch 3/40\n",
      "133/133 [==============================] - 89s 669ms/step - loss: 1.2216 - categorical_accuracy: 0.5018 - val_loss: 0.9392 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.47000 to 0.64000, saving model to model_9/model.h5\n",
      "Epoch 4/40\n",
      "133/133 [==============================] - 89s 669ms/step - loss: 0.9951 - categorical_accuracy: 0.5945 - val_loss: 1.7473 - val_categorical_accuracy: 0.3800\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.64000\n",
      "Epoch 5/40\n",
      "133/133 [==============================] - 89s 667ms/step - loss: 0.9505 - categorical_accuracy: 0.6160 - val_loss: 1.0339 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.64000\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 6/40\n",
      "133/133 [==============================] - 89s 672ms/step - loss: 0.7541 - categorical_accuracy: 0.6917 - val_loss: 0.7873 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy improved from 0.64000 to 0.71000, saving model to model_9/model.h5\n",
      "Epoch 7/40\n",
      "133/133 [==============================] - 89s 666ms/step - loss: 0.6833 - categorical_accuracy: 0.7308 - val_loss: 0.9361 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.71000\n",
      "Epoch 8/40\n",
      "133/133 [==============================] - 89s 671ms/step - loss: 0.5622 - categorical_accuracy: 0.8075 - val_loss: 0.9021 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.71000\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 9/40\n",
      "133/133 [==============================] - 90s 673ms/step - loss: 0.5683 - categorical_accuracy: 0.8035 - val_loss: 0.8114 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.71000 to 0.72000, saving model to model_9/model.h5\n",
      "Epoch 10/40\n",
      "133/133 [==============================] - 89s 672ms/step - loss: 0.5273 - categorical_accuracy: 0.8060 - val_loss: 0.6292 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.72000 to 0.77000, saving model to model_9/model.h5\n",
      "Epoch 11/40\n",
      "133/133 [==============================] - 89s 672ms/step - loss: 0.4454 - categorical_accuracy: 0.8441 - val_loss: 0.8245 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.77000\n",
      "Epoch 12/40\n",
      "133/133 [==============================] - 89s 672ms/step - loss: 0.4396 - categorical_accuracy: 0.8321 - val_loss: 0.7382 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.77000\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 13/40\n",
      "133/133 [==============================] - 88s 663ms/step - loss: 0.3730 - categorical_accuracy: 0.8782 - val_loss: 0.5563 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00013: val_categorical_accuracy improved from 0.77000 to 0.83000, saving model to model_9/model.h5\n",
      "Epoch 14/40\n",
      "133/133 [==============================] - 89s 670ms/step - loss: 0.3486 - categorical_accuracy: 0.8857 - val_loss: 0.5913 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.83000\n",
      "Epoch 15/40\n",
      "133/133 [==============================] - 90s 675ms/step - loss: 0.3422 - categorical_accuracy: 0.8897 - val_loss: 0.6770 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.83000\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 16/40\n",
      "133/133 [==============================] - 90s 675ms/step - loss: 0.2967 - categorical_accuracy: 0.9073 - val_loss: 0.6148 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00016: val_categorical_accuracy did not improve from 0.83000\n",
      "Epoch 17/40\n",
      "133/133 [==============================] - 89s 671ms/step - loss: 0.3329 - categorical_accuracy: 0.9007 - val_loss: 0.5585 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.83000\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 18/40\n",
      "133/133 [==============================] - 90s 673ms/step - loss: 0.3262 - categorical_accuracy: 0.8932 - val_loss: 0.5603 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.83000\n",
      "Epoch 19/40\n",
      "133/133 [==============================] - 89s 667ms/step - loss: 0.3321 - categorical_accuracy: 0.8797 - val_loss: 0.5988 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.83000\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 20/40\n",
      "133/133 [==============================] - 90s 673ms/step - loss: 0.3088 - categorical_accuracy: 0.9068 - val_loss: 0.5930 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.83000\n",
      "Epoch 21/40\n",
      "133/133 [==============================] - 89s 673ms/step - loss: 0.2398 - categorical_accuracy: 0.9253 - val_loss: 0.5787 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00021: val_categorical_accuracy did not improve from 0.83000\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 22/40\n",
      "133/133 [==============================] - 89s 671ms/step - loss: 0.2861 - categorical_accuracy: 0.9158 - val_loss: 0.5740 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00022: val_categorical_accuracy did not improve from 0.83000\n",
      "Epoch 23/40\n",
      "133/133 [==============================] - 89s 673ms/step - loss: 0.2919 - categorical_accuracy: 0.9158 - val_loss: 0.5648 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00023: val_categorical_accuracy did not improve from 0.83000\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 24/40\n",
      "133/133 [==============================] - 90s 677ms/step - loss: 0.2950 - categorical_accuracy: 0.9068 - val_loss: 0.5599 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00024: val_categorical_accuracy did not improve from 0.83000\n",
      "Epoch 25/40\n",
      "133/133 [==============================] - 90s 675ms/step - loss: 0.2428 - categorical_accuracy: 0.9283 - val_loss: 0.5590 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00025: val_categorical_accuracy did not improve from 0.83000\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 26/40\n",
      "133/133 [==============================] - 90s 676ms/step - loss: 0.3012 - categorical_accuracy: 0.9148 - val_loss: 0.5601 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00026: val_categorical_accuracy did not improve from 0.83000\n",
      "Epoch 27/40\n",
      "133/133 [==============================] - 90s 675ms/step - loss: 0.2837 - categorical_accuracy: 0.9088 - val_loss: 0.5615 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00027: val_categorical_accuracy did not improve from 0.83000\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 28/40\n",
      "133/133 [==============================] - 90s 676ms/step - loss: 0.2902 - categorical_accuracy: 0.9128 - val_loss: 0.5613 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00028: val_categorical_accuracy did not improve from 0.83000\n",
      "Epoch 29/40\n",
      "133/133 [==============================] - 90s 679ms/step - loss: 0.2728 - categorical_accuracy: 0.9033 - val_loss: 0.5676 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00029: val_categorical_accuracy did not improve from 0.83000\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 30/40\n",
      "133/133 [==============================] - 89s 670ms/step - loss: 0.2621 - categorical_accuracy: 0.9298 - val_loss: 0.5557 - val_categorical_accuracy: 0.8200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00030: val_categorical_accuracy did not improve from 0.83000\n",
      "Epoch 31/40\n",
      "133/133 [==============================] - 90s 680ms/step - loss: 0.2731 - categorical_accuracy: 0.9223 - val_loss: 0.5534 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00031: val_categorical_accuracy did not improve from 0.83000\n",
      "Epoch 32/40\n",
      "133/133 [==============================] - 90s 680ms/step - loss: 0.3030 - categorical_accuracy: 0.9113 - val_loss: 0.5625 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00032: val_categorical_accuracy did not improve from 0.83000\n",
      "Epoch 33/40\n",
      "133/133 [==============================] - 90s 677ms/step - loss: 0.2638 - categorical_accuracy: 0.9158 - val_loss: 0.5598 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00033: val_categorical_accuracy did not improve from 0.83000\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 34/40\n",
      "133/133 [==============================] - 90s 677ms/step - loss: 0.2771 - categorical_accuracy: 0.9173 - val_loss: 0.5601 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00034: val_categorical_accuracy did not improve from 0.83000\n",
      "Epoch 35/40\n",
      "133/133 [==============================] - 90s 677ms/step - loss: 0.2814 - categorical_accuracy: 0.9233 - val_loss: 0.5557 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00035: val_categorical_accuracy did not improve from 0.83000\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 36/40\n",
      "133/133 [==============================] - 90s 677ms/step - loss: 0.2405 - categorical_accuracy: 0.9218 - val_loss: 0.5515 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00036: val_categorical_accuracy did not improve from 0.83000\n",
      "Epoch 37/40\n",
      "133/133 [==============================] - 89s 668ms/step - loss: 0.2607 - categorical_accuracy: 0.9293 - val_loss: 0.5589 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00037: val_categorical_accuracy did not improve from 0.83000\n",
      "Epoch 38/40\n",
      "133/133 [==============================] - 90s 673ms/step - loss: 0.2767 - categorical_accuracy: 0.9148 - val_loss: 0.5570 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00038: val_categorical_accuracy did not improve from 0.83000\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n",
      "Epoch 39/40\n",
      "133/133 [==============================] - 90s 677ms/step - loss: 0.2680 - categorical_accuracy: 0.9308 - val_loss: 0.5579 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00039: val_categorical_accuracy did not improve from 0.83000\n",
      "Epoch 40/40\n",
      "133/133 [==============================] - 89s 669ms/step - loss: 0.2940 - categorical_accuracy: 0.9133 - val_loss: 0.5539 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00040: val_categorical_accuracy did not improve from 0.83000\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 3.051757957450718e-08.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f502a7a2b00>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_9' + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "#filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "filepath = model_name + 'model.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 10\n",
    "\n",
    "In the previous model - Model 9, we have got validation accuracy of 83%. For Model 10, we will modify the inputs a little. Until Model 9, we were using images of size 120 X 160, for Model 10, we will try using images of dimension 120 X 120 to see if that helps the model learn better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 30, 120, 120, 8)   1736      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 15, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 15, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 15, 60, 60, 16)    6928      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 15, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 7, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 7, 30, 30, 32)     13856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 7, 30, 30, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 3, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 21600)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               5529856   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 5,591,901\n",
      "Trainable params: 5,590,973\n",
      "Non-trainable params: 928\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(\n",
    "    8, (3,3,3), input_shape=(30,120,120,3),\n",
    "    padding='same'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(8, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(16, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv3D(16, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(32, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "optimiser = Adam()\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 3s - loss: 1.6440 - categorical_accuracy: 0.3160"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 250s 2s/step - loss: 1.6474 - categorical_accuracy: 0.3128 - val_loss: 2.2583 - val_categorical_accuracy: 0.2300\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.23000, saving model to model_10/model.h5\n",
      "Epoch 2/40\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.4500 - categorical_accuracy: 0.3969"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:50: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 72s 542ms/step - loss: 1.4555 - categorical_accuracy: 0.3950 - val_loss: 1.4106 - val_categorical_accuracy: 0.3900\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.23000 to 0.39000, saving model to model_10/model.h5\n",
      "Epoch 3/40\n",
      "133/133 [==============================] - 71s 532ms/step - loss: 1.2659 - categorical_accuracy: 0.4782 - val_loss: 1.5674 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.39000 to 0.45000, saving model to model_10/model.h5\n",
      "Epoch 4/40\n",
      "133/133 [==============================] - 74s 554ms/step - loss: 1.0994 - categorical_accuracy: 0.5634 - val_loss: 1.3149 - val_categorical_accuracy: 0.5100\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.45000 to 0.51000, saving model to model_10/model.h5\n",
      "Epoch 5/40\n",
      "133/133 [==============================] - 72s 544ms/step - loss: 0.9934 - categorical_accuracy: 0.5985 - val_loss: 1.2421 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.51000 to 0.53000, saving model to model_10/model.h5\n",
      "Epoch 6/40\n",
      "133/133 [==============================] - 73s 547ms/step - loss: 0.9347 - categorical_accuracy: 0.6531 - val_loss: 1.0734 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy improved from 0.53000 to 0.57000, saving model to model_10/model.h5\n",
      "Epoch 7/40\n",
      "133/133 [==============================] - 72s 540ms/step - loss: 0.8049 - categorical_accuracy: 0.6947 - val_loss: 0.9023 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.57000 to 0.63000, saving model to model_10/model.h5\n",
      "Epoch 8/40\n",
      "133/133 [==============================] - 73s 547ms/step - loss: 0.7274 - categorical_accuracy: 0.7123 - val_loss: 2.3022 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.63000\n",
      "Epoch 9/40\n",
      "133/133 [==============================] - 72s 541ms/step - loss: 0.6976 - categorical_accuracy: 0.7193 - val_loss: 0.9869 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.63000 to 0.65000, saving model to model_10/model.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 10/40\n",
      "133/133 [==============================] - 73s 547ms/step - loss: 0.5637 - categorical_accuracy: 0.8075 - val_loss: 0.9793 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.65000\n",
      "Epoch 11/40\n",
      "133/133 [==============================] - 73s 549ms/step - loss: 0.5145 - categorical_accuracy: 0.8306 - val_loss: 0.9075 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00011: val_categorical_accuracy improved from 0.65000 to 0.66000, saving model to model_10/model.h5\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 12/40\n",
      "133/133 [==============================] - 73s 547ms/step - loss: 0.4520 - categorical_accuracy: 0.8406 - val_loss: 0.9040 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.66000\n",
      "Epoch 13/40\n",
      "133/133 [==============================] - 71s 537ms/step - loss: 0.3756 - categorical_accuracy: 0.8657 - val_loss: 0.7481 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00013: val_categorical_accuracy improved from 0.66000 to 0.74000, saving model to model_10/model.h5\n",
      "Epoch 14/40\n",
      "133/133 [==============================] - 73s 553ms/step - loss: 0.3698 - categorical_accuracy: 0.8937 - val_loss: 1.0791 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.74000\n",
      "Epoch 15/40\n",
      "133/133 [==============================] - 73s 547ms/step - loss: 0.3895 - categorical_accuracy: 0.8652 - val_loss: 0.7854 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.74000\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 16/40\n",
      "133/133 [==============================] - 72s 543ms/step - loss: 0.3295 - categorical_accuracy: 0.8857 - val_loss: 0.7035 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00016: val_categorical_accuracy improved from 0.74000 to 0.81000, saving model to model_10/model.h5\n",
      "Epoch 17/40\n",
      "133/133 [==============================] - 72s 543ms/step - loss: 0.3423 - categorical_accuracy: 0.8697 - val_loss: 0.7798 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 18/40\n",
      "133/133 [==============================] - 73s 549ms/step - loss: 0.3365 - categorical_accuracy: 0.8802 - val_loss: 0.6747 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 19/40\n",
      "133/133 [==============================] - 73s 548ms/step - loss: 0.3474 - categorical_accuracy: 0.8887 - val_loss: 0.7404 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 20/40\n",
      "133/133 [==============================] - 74s 556ms/step - loss: 0.2545 - categorical_accuracy: 0.9293 - val_loss: 0.9060 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 21/40\n",
      "133/133 [==============================] - 72s 540ms/step - loss: 0.2962 - categorical_accuracy: 0.9073 - val_loss: 0.7392 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00021: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 22/40\n",
      "133/133 [==============================] - 72s 542ms/step - loss: 0.3000 - categorical_accuracy: 0.9043 - val_loss: 0.7337 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00022: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 23/40\n",
      "133/133 [==============================] - 73s 548ms/step - loss: 0.2878 - categorical_accuracy: 0.9188 - val_loss: 0.7059 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00023: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 24/40\n",
      "133/133 [==============================] - 73s 550ms/step - loss: 0.2531 - categorical_accuracy: 0.9093 - val_loss: 0.7158 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00024: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 25/40\n",
      "133/133 [==============================] - 73s 546ms/step - loss: 0.2673 - categorical_accuracy: 0.9148 - val_loss: 0.7245 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00025: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 26/40\n",
      "133/133 [==============================] - 72s 539ms/step - loss: 0.2765 - categorical_accuracy: 0.9083 - val_loss: 0.7091 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00026: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 27/40\n",
      "133/133 [==============================] - 72s 539ms/step - loss: 0.2929 - categorical_accuracy: 0.8962 - val_loss: 0.7112 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00027: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 28/40\n",
      "133/133 [==============================] - 73s 546ms/step - loss: 0.2836 - categorical_accuracy: 0.9143 - val_loss: 0.7099 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00028: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 29/40\n",
      "133/133 [==============================] - 71s 534ms/step - loss: 0.2328 - categorical_accuracy: 0.9313 - val_loss: 0.7150 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00029: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 30/40\n",
      "133/133 [==============================] - 72s 540ms/step - loss: 0.2662 - categorical_accuracy: 0.9193 - val_loss: 0.7212 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00030: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 31/40\n",
      "133/133 [==============================] - 71s 536ms/step - loss: 0.2865 - categorical_accuracy: 0.9053 - val_loss: 0.7171 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00031: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 32/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 72s 544ms/step - loss: 0.2417 - categorical_accuracy: 0.9308 - val_loss: 0.7185 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00032: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 33/40\n",
      "133/133 [==============================] - 71s 534ms/step - loss: 0.2774 - categorical_accuracy: 0.9103 - val_loss: 0.7210 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00033: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 34/40\n",
      "133/133 [==============================] - 70s 530ms/step - loss: 0.2306 - categorical_accuracy: 0.9263 - val_loss: 0.7179 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00034: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 35/40\n",
      "133/133 [==============================] - 71s 533ms/step - loss: 0.3169 - categorical_accuracy: 0.8892 - val_loss: 0.7141 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00035: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 36/40\n",
      "133/133 [==============================] - 71s 533ms/step - loss: 0.2421 - categorical_accuracy: 0.9373 - val_loss: 0.7170 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00036: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 37/40\n",
      "133/133 [==============================] - 71s 536ms/step - loss: 0.2856 - categorical_accuracy: 0.9058 - val_loss: 0.7152 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00037: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 38/40\n",
      "133/133 [==============================] - 71s 537ms/step - loss: 0.2385 - categorical_accuracy: 0.9308 - val_loss: 0.7179 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00038: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 39/40\n",
      "133/133 [==============================] - 71s 536ms/step - loss: 0.2588 - categorical_accuracy: 0.9414 - val_loss: 0.7127 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00039: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 40/40\n",
      "133/133 [==============================] - 72s 543ms/step - loss: 0.2804 - categorical_accuracy: 0.9053 - val_loss: 0.7069 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00040: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 6.103515914901436e-08.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7dafbc1550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_10' + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "#filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "filepath = model_name + 'model.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran training for 40 epochs, but our best validation accuracy reached only 81%. So, Model 9 is still our best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_23 (Conv3D)           (None, 30, 120, 160, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 30, 120, 160, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 30, 120, 160, 8)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_24 (Conv3D)           (None, 30, 120, 160, 8)   1736      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 30, 120, 160, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 30, 120, 160, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_15 (MaxPooling (None, 15, 60, 80, 8)     0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 15, 60, 80, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_25 (Conv3D)           (None, 15, 60, 80, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 15, 60, 80, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 15, 60, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_26 (Conv3D)           (None, 15, 60, 80, 16)    6928      \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 15, 60, 80, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 15, 60, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_16 (MaxPooling (None, 7, 30, 40, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_27 (Conv3D)           (None, 7, 30, 40, 32)     13856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 7, 30, 40, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 7, 30, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_17 (MaxPooling (None, 3, 15, 20, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 3, 15, 20, 32)     0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 28800)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               7373056   \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 7,435,101\n",
      "Trainable params: 7,434,173\n",
      "Non-trainable params: 928\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(\n",
    "    8, (3,3,3), input_shape=(30,120,160,3),\n",
    "    padding='same'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(8, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(16, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv3D(16, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(32, (3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# model summary\n",
    "optimiser = Adam()\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "127/133 [===========================>..] - ETA: 3s - loss: 1.5830 - categorical_accuracy: 0.3323"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:48: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 94s 707ms/step - loss: 1.5763 - categorical_accuracy: 0.3363 - val_loss: 1.6588 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.44000, saving model to model_11/model.h5\n",
      "Epoch 2/60\n",
      "133/133 [==============================] - 88s 660ms/step - loss: 1.3058 - categorical_accuracy: 0.4471 - val_loss: 1.5176 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.44000 to 0.48000, saving model to model_11/model.h5\n",
      "Epoch 3/60\n",
      "133/133 [==============================] - 88s 660ms/step - loss: 1.1324 - categorical_accuracy: 0.5394 - val_loss: 1.5819 - val_categorical_accuracy: 0.4100\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.48000\n",
      "Epoch 4/60\n",
      "133/133 [==============================] - 88s 663ms/step - loss: 1.0594 - categorical_accuracy: 0.5769 - val_loss: 1.0434 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.48000 to 0.61000, saving model to model_11/model.h5\n",
      "Epoch 5/60\n",
      "133/133 [==============================] - 88s 663ms/step - loss: 1.0629 - categorical_accuracy: 0.5709 - val_loss: 0.9967 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.61000\n",
      "Epoch 6/60\n",
      "133/133 [==============================] - 88s 664ms/step - loss: 0.8553 - categorical_accuracy: 0.6501 - val_loss: 1.6115 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.61000\n",
      "Epoch 7/60\n",
      "133/133 [==============================] - 88s 661ms/step - loss: 0.8466 - categorical_accuracy: 0.6722 - val_loss: 1.3813 - val_categorical_accuracy: 0.5600\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.61000\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "Epoch 8/60\n",
      "133/133 [==============================] - 90s 679ms/step - loss: 0.8029 - categorical_accuracy: 0.7148 - val_loss: 0.7864 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy improved from 0.61000 to 0.67000, saving model to model_11/model.h5\n",
      "Epoch 9/60\n",
      "133/133 [==============================] - 90s 679ms/step - loss: 0.6529 - categorical_accuracy: 0.7684 - val_loss: 0.6949 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.67000 to 0.75000, saving model to model_11/model.h5\n",
      "Epoch 10/60\n",
      "133/133 [==============================] - 89s 671ms/step - loss: 0.6108 - categorical_accuracy: 0.7684 - val_loss: 0.9870 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.75000\n",
      "Epoch 11/60\n",
      "133/133 [==============================] - 90s 675ms/step - loss: 0.5775 - categorical_accuracy: 0.7915 - val_loss: 0.9618 - val_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.75000\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "Epoch 12/60\n",
      "133/133 [==============================] - 90s 678ms/step - loss: 0.5013 - categorical_accuracy: 0.8436 - val_loss: 0.6810 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00012: val_categorical_accuracy improved from 0.75000 to 0.75000, saving model to model_11/model.h5\n",
      "Epoch 13/60\n",
      "133/133 [==============================] - 89s 672ms/step - loss: 0.5057 - categorical_accuracy: 0.8110 - val_loss: 0.9229 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.75000\n",
      "Epoch 14/60\n",
      "133/133 [==============================] - 90s 678ms/step - loss: 0.4460 - categorical_accuracy: 0.8541 - val_loss: 0.6565 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.75000\n",
      "Epoch 15/60\n",
      "133/133 [==============================] - 90s 674ms/step - loss: 0.4489 - categorical_accuracy: 0.8526 - val_loss: 0.9099 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.75000\n",
      "Epoch 16/60\n",
      "133/133 [==============================] - 90s 678ms/step - loss: 0.4487 - categorical_accuracy: 0.8366 - val_loss: 0.6066 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00016: val_categorical_accuracy improved from 0.75000 to 0.77000, saving model to model_11/model.h5\n",
      "Epoch 17/60\n",
      "133/133 [==============================] - 89s 673ms/step - loss: 0.3218 - categorical_accuracy: 0.8977 - val_loss: 0.7355 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.77000\n",
      "Epoch 18/60\n",
      "133/133 [==============================] - 91s 684ms/step - loss: 0.3752 - categorical_accuracy: 0.8807 - val_loss: 0.9798 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.77000\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
      "Epoch 19/60\n",
      "133/133 [==============================] - 89s 673ms/step - loss: 0.2896 - categorical_accuracy: 0.9053 - val_loss: 0.6834 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.77000\n",
      "Epoch 20/60\n",
      "133/133 [==============================] - 90s 678ms/step - loss: 0.2914 - categorical_accuracy: 0.8992 - val_loss: 0.7638 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.77000\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00024009999469853935.\n",
      "Epoch 21/60\n",
      "133/133 [==============================] - 90s 680ms/step - loss: 0.2677 - categorical_accuracy: 0.9248 - val_loss: 0.7031 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00021: val_categorical_accuracy improved from 0.77000 to 0.78000, saving model to model_11/model.h5\n",
      "Epoch 22/60\n",
      "133/133 [==============================] - 90s 675ms/step - loss: 0.2332 - categorical_accuracy: 0.9308 - val_loss: 0.6106 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00022: val_categorical_accuracy improved from 0.78000 to 0.79000, saving model to model_11/model.h5\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00016806999628897755.\n",
      "Epoch 23/60\n",
      "133/133 [==============================] - 90s 678ms/step - loss: 0.2126 - categorical_accuracy: 0.9429 - val_loss: 0.6430 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00023: val_categorical_accuracy did not improve from 0.79000\n",
      "Epoch 24/60\n",
      "133/133 [==============================] - 89s 672ms/step - loss: 0.1958 - categorical_accuracy: 0.9504 - val_loss: 0.6138 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00024: val_categorical_accuracy did not improve from 0.79000\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00011764899536501615.\n",
      "Epoch 25/60\n",
      "133/133 [==============================] - 90s 675ms/step - loss: 0.1719 - categorical_accuracy: 0.9504 - val_loss: 0.5952 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00025: val_categorical_accuracy improved from 0.79000 to 0.80000, saving model to model_11/model.h5\n",
      "Epoch 26/60\n",
      "133/133 [==============================] - 90s 680ms/step - loss: 0.1887 - categorical_accuracy: 0.9549 - val_loss: 0.6123 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00026: val_categorical_accuracy did not improve from 0.80000\n",
      "Epoch 27/60\n",
      "133/133 [==============================] - 90s 673ms/step - loss: 0.1603 - categorical_accuracy: 0.9624 - val_loss: 0.5705 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00027: val_categorical_accuracy did not improve from 0.80000\n",
      "Epoch 28/60\n",
      "133/133 [==============================] - 90s 678ms/step - loss: 0.1813 - categorical_accuracy: 0.9444 - val_loss: 0.6010 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00028: val_categorical_accuracy did not improve from 0.80000\n",
      "Epoch 29/60\n",
      "133/133 [==============================] - 90s 676ms/step - loss: 0.1657 - categorical_accuracy: 0.9519 - val_loss: 0.6408 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00029: val_categorical_accuracy did not improve from 0.80000\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 8.235429777414538e-05.\n",
      "Epoch 30/60\n",
      "133/133 [==============================] - 90s 674ms/step - loss: 0.1608 - categorical_accuracy: 0.9564 - val_loss: 0.5880 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00030: val_categorical_accuracy did not improve from 0.80000\n",
      "Epoch 31/60\n",
      "133/133 [==============================] - 91s 684ms/step - loss: 0.1795 - categorical_accuracy: 0.9479 - val_loss: 0.5786 - val_categorical_accuracy: 0.8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00031: val_categorical_accuracy improved from 0.80000 to 0.81000, saving model to model_11/model.h5\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.76480058953166e-05.\n",
      "Epoch 32/60\n",
      "133/133 [==============================] - 90s 679ms/step - loss: 0.1702 - categorical_accuracy: 0.9549 - val_loss: 0.5777 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00032: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 33/60\n",
      "133/133 [==============================] - 90s 674ms/step - loss: 0.1753 - categorical_accuracy: 0.9549 - val_loss: 0.5978 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00033: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 4.0353603617404586e-05.\n",
      "Epoch 34/60\n",
      "133/133 [==============================] - 90s 677ms/step - loss: 0.1457 - categorical_accuracy: 0.9654 - val_loss: 0.6112 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00034: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 35/60\n",
      "133/133 [==============================] - 90s 680ms/step - loss: 0.1696 - categorical_accuracy: 0.9539 - val_loss: 0.5647 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00035: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 36/60\n",
      "133/133 [==============================] - 90s 678ms/step - loss: 0.1643 - categorical_accuracy: 0.9524 - val_loss: 0.5976 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00036: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 37/60\n",
      "133/133 [==============================] - 90s 675ms/step - loss: 0.1471 - categorical_accuracy: 0.9599 - val_loss: 0.6062 - val_categorical_accuracy: 0.7600\n",
      "\n",
      "Epoch 00037: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 2.8247522277524694e-05.\n",
      "Epoch 38/60\n",
      "133/133 [==============================] - 90s 680ms/step - loss: 0.1488 - categorical_accuracy: 0.9594 - val_loss: 0.5924 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00038: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 39/60\n",
      "133/133 [==============================] - 90s 676ms/step - loss: 0.1495 - categorical_accuracy: 0.9594 - val_loss: 0.5785 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00039: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.977326610358432e-05.\n",
      "Epoch 40/60\n",
      "133/133 [==============================] - 89s 673ms/step - loss: 0.1460 - categorical_accuracy: 0.9639 - val_loss: 0.5866 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00040: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 41/60\n",
      "133/133 [==============================] - 89s 671ms/step - loss: 0.1519 - categorical_accuracy: 0.9519 - val_loss: 0.5891 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00041: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.3841286272509023e-05.\n",
      "Epoch 42/60\n",
      "133/133 [==============================] - 90s 678ms/step - loss: 0.1457 - categorical_accuracy: 0.9474 - val_loss: 0.5871 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00042: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 43/60\n",
      "133/133 [==============================] - 91s 683ms/step - loss: 0.1452 - categorical_accuracy: 0.9654 - val_loss: 0.5854 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00043: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 9.688900263427058e-06.\n",
      "Epoch 44/60\n",
      "133/133 [==============================] - 90s 680ms/step - loss: 0.1525 - categorical_accuracy: 0.9549 - val_loss: 0.5765 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00044: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 45/60\n",
      "133/133 [==============================] - 91s 686ms/step - loss: 0.1574 - categorical_accuracy: 0.9569 - val_loss: 0.5773 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00045: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 6.782229866075795e-06.\n",
      "Epoch 46/60\n",
      "133/133 [==============================] - 91s 683ms/step - loss: 0.2119 - categorical_accuracy: 0.9343 - val_loss: 0.5799 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00046: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 47/60\n",
      "133/133 [==============================] - 90s 677ms/step - loss: 0.1138 - categorical_accuracy: 0.9719 - val_loss: 0.5738 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00047: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 4.747560842588427e-06.\n",
      "Epoch 48/60\n",
      "133/133 [==============================] - 90s 677ms/step - loss: 0.1284 - categorical_accuracy: 0.9639 - val_loss: 0.5839 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00048: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 49/60\n",
      "133/133 [==============================] - 90s 673ms/step - loss: 0.1350 - categorical_accuracy: 0.9684 - val_loss: 0.5773 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00049: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 3.3232926853088427e-06.\n",
      "Epoch 50/60\n",
      "133/133 [==============================] - 91s 682ms/step - loss: 0.1018 - categorical_accuracy: 0.9729 - val_loss: 0.5784 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00050: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 51/60\n",
      "133/133 [==============================] - 90s 680ms/step - loss: 0.1591 - categorical_accuracy: 0.9404 - val_loss: 0.5842 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00051: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 2.3263049115485044e-06.\n",
      "Epoch 52/60\n",
      "133/133 [==============================] - 90s 678ms/step - loss: 0.1395 - categorical_accuracy: 0.9699 - val_loss: 0.5790 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00052: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 53/60\n",
      "133/133 [==============================] - 90s 678ms/step - loss: 0.1236 - categorical_accuracy: 0.9684 - val_loss: 0.5767 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00053: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 1.6284134062516385e-06.\n",
      "Epoch 54/60\n",
      "133/133 [==============================] - 89s 672ms/step - loss: 0.1508 - categorical_accuracy: 0.9639 - val_loss: 0.5765 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00054: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 55/60\n",
      "133/133 [==============================] - 91s 681ms/step - loss: 0.1367 - categorical_accuracy: 0.9609 - val_loss: 0.5781 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00055: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1.1398894002923043e-06.\n",
      "Epoch 56/60\n",
      "133/133 [==============================] - 90s 677ms/step - loss: 0.1340 - categorical_accuracy: 0.9614 - val_loss: 0.5753 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00056: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 57/60\n",
      "133/133 [==============================] - 90s 674ms/step - loss: 0.1237 - categorical_accuracy: 0.9654 - val_loss: 0.5786 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00057: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 7.979225642884557e-07.\n",
      "Epoch 58/60\n",
      "133/133 [==============================] - 91s 680ms/step - loss: 0.1347 - categorical_accuracy: 0.9669 - val_loss: 0.5817 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00058: val_categorical_accuracy did not improve from 0.81000\n",
      "Epoch 59/60\n",
      "133/133 [==============================] - 89s 673ms/step - loss: 0.1851 - categorical_accuracy: 0.9459 - val_loss: 0.5793 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00059: val_categorical_accuracy did not improve from 0.81000\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 5.585457870438403e-07.\n",
      "Epoch 60/60\n",
      "133/133 [==============================] - 90s 675ms/step - loss: 0.1496 - categorical_accuracy: 0.9539 - val_loss: 0.5752 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00060: val_categorical_accuracy did not improve from 0.81000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd2e103d9e8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model_name = 'model_11' + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "#filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "filepath = model_name + 'model.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=2, cooldown=1, verbose=1)\n",
    "callbacks_list = [checkpoint, LR]\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 12 : Conv 2D + RNN(LSTM/GRU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Reshape, Lambda\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Conv2D, LSTM, TimeDistributed, Flatten, Input, GRU, GlobalAveragePooling2D\n",
    "from keras.applications import VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_per_epoch 133\n",
      "validation_steps 20\n"
     ]
    }
   ],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "print ('steps_per_epoch %d' % steps_per_epoch)\n",
    "print ('validation_steps %d' % validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_15 (TimeDis (None, None, 7, 7, 64)    28008     \n",
      "_________________________________________________________________\n",
      "time_distributed_16 (TimeDis (None, None, 3136)        0         \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (None, 256)               2605824   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 2,635,117\n",
      "Trainable params: 2,634,829\n",
      "Non-trainable params: 288\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape = (120, 120, 3)\n",
    "model = Sequential()\n",
    "x_input = Input(shape= input_shape)\n",
    "\n",
    "x_output = Conv2D(filters=8, kernel_size=(3,3), padding='same')(x_input)\n",
    "x_output = BatchNormalization()(x_output)\n",
    "x_output = Activation('relu')(x_output)\n",
    "x_output = Conv2D(filters=8, kernel_size=(3,3),padding='same')(x_output)\n",
    "x_output = BatchNormalization()(x_output)\n",
    "x_output = Activation('relu')(x_output)\n",
    "x_output = MaxPooling2D(pool_size=(2, 2))(x_output)\n",
    "x_output = Dropout(0.25)(x_output)\n",
    "\n",
    "x_output = Conv2D(filters=16, kernel_size=(3,3), padding='same')(x_output)\n",
    "x_output = BatchNormalization()(x_output)\n",
    "x_output = Activation('relu')(x_output)\n",
    "x_output = Conv2D(filters=16, kernel_size=(3,3), )(x_output)\n",
    "x_output = BatchNormalization()(x_output)\n",
    "x_output = Activation('relu')(x_output)\n",
    "x_output = MaxPooling2D(pool_size=(2, 2))(x_output)\n",
    "x_output = Dropout(0.25)(x_output)\n",
    "\n",
    "\n",
    "x_output = Conv2D(filters=32, kernel_size=(3,3), padding='same')(x_output)\n",
    "x_output = BatchNormalization()(x_output)\n",
    "x_output = Activation('relu')(x_output)\n",
    "x_output = MaxPooling2D(pool_size=(2, 2))(x_output)\n",
    "x_output = Dropout(0.25)(x_output)\n",
    "\n",
    "x_output = Conv2D(filters=64, kernel_size=(3,3), padding='same')(x_output)\n",
    "x_output = BatchNormalization()(x_output)\n",
    "x_output = Activation('relu')(x_output)\n",
    "x_output = MaxPooling2D(pool_size=(2, 2))(x_output)\n",
    "x_output = Dropout(0.25)(x_output)\n",
    "\n",
    "base_model = Model(x_input, x_output)\n",
    "\n",
    "model.add(TimeDistributed(base_model, input_shape=base_model.input_shape))\n",
    "model.add(TimeDistributed(Flatten(input_shape=base_model.input_shape[1:])))\n",
    "model.add(GRU(256, activation='relu', recurrent_activation='hard_sigmoid', dropout=0.2))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "optimiser = Adam()\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "model_name = 'model_13' + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "#filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "filepath = model_name + 'model.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n",
    "callbacks_list = [checkpoint, LR]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 1s - loss: 8.2109 - categorical_accuracy: 0.2397"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 75s 562ms/step - loss: 8.1126 - categorical_accuracy: 0.2386 - val_loss: 1.7450 - val_categorical_accuracy: 0.2600\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.26000, saving model to model_13/model.h5\n",
      "Epoch 2/5\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.3871 - categorical_accuracy: 0.3740"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:50: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 70s 525ms/step - loss: 1.3910 - categorical_accuracy: 0.3684 - val_loss: 1.4279 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.26000 to 0.40000, saving model to model_13/model.h5\n",
      "Epoch 3/5\n",
      "133/133 [==============================] - 72s 538ms/step - loss: 1.2543 - categorical_accuracy: 0.4436 - val_loss: 1.6524 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.40000\n",
      "Epoch 4/5\n",
      "133/133 [==============================] - 71s 530ms/step - loss: 1.1200 - categorical_accuracy: 0.5343 - val_loss: 1.4255 - val_categorical_accuracy: 0.4000\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.40000\n",
      "Epoch 5/5\n",
      "133/133 [==============================] - 71s 533ms/step - loss: 1.1014 - categorical_accuracy: 0.5409 - val_loss: 1.9618 - val_categorical_accuracy: 0.3700\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.40000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc4d2dd99e8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=5, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121/133 [==========================>...] - ETA: 6s - loss: 1.0325 - categorical_accuracy: 0.5554"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:50: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 76s 569ms/step - loss: 1.0144 - categorical_accuracy: 0.5624 - val_loss: 1.7454 - val_categorical_accuracy: 0.4300\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from 0.40000 to 0.43000, saving model to model_13/model.h5\n",
      "Epoch 2/5\n",
      "133/133 [==============================] - 72s 541ms/step - loss: 0.9608 - categorical_accuracy: 0.5935 - val_loss: 1.4348 - val_categorical_accuracy: 0.4800\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.43000 to 0.48000, saving model to model_13/model.h5\n",
      "Epoch 3/5\n",
      "133/133 [==============================] - 71s 534ms/step - loss: 0.8789 - categorical_accuracy: 0.6266 - val_loss: 1.3274 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.48000\n",
      "Epoch 4/5\n",
      "133/133 [==============================] - 71s 537ms/step - loss: 0.7903 - categorical_accuracy: 0.6622 - val_loss: 1.4391 - val_categorical_accuracy: 0.4400\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.48000\n",
      "Epoch 5/5\n",
      "133/133 [==============================] - 70s 529ms/step - loss: 0.7614 - categorical_accuracy: 0.7007 - val_loss: 0.8987 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.48000 to 0.64000, saving model to model_13/model.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc4d2dd98d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=5, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/133 [========================>.....] - ETA: 11s - loss: 0.7635 - categorical_accuracy: 0.7027"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:50: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 78s 589ms/step - loss: 0.7606 - categorical_accuracy: 0.7043 - val_loss: 1.1718 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy did not improve from 0.64000\n",
      "Epoch 2/5\n",
      "133/133 [==============================] - 70s 529ms/step - loss: 0.6546 - categorical_accuracy: 0.7479 - val_loss: 0.9642 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy did not improve from 0.64000\n",
      "Epoch 3/5\n",
      "133/133 [==============================] - 73s 550ms/step - loss: 0.6452 - categorical_accuracy: 0.7539 - val_loss: 1.0016 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.64000\n",
      "Epoch 4/5\n",
      "133/133 [==============================] - 70s 526ms/step - loss: 0.6433 - categorical_accuracy: 0.7419 - val_loss: 1.2722 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.64000\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 5/5\n",
      "133/133 [==============================] - 73s 547ms/step - loss: 0.5408 - categorical_accuracy: 0.7885 - val_loss: 1.4934 - val_categorical_accuracy: 0.4500\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.64000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc4d2dd9ac8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=5, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result of Conv2d + LRU => we got 64% accurancy on validation data and approx 78% on train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 13 :  Transfer learning +  RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (None, None, 3, 3, 512)   20024384  \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, None, 4608)        0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 256)               3736320   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 23,761,989\n",
      "Trainable params: 3,737,605\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(120, 120, 3))\n",
    "\n",
    "# freeze the layers in base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "model = Sequential()\n",
    "\n",
    "model.add(TimeDistributed(base_model, input_shape=base_model.input_shape))\n",
    "model.add(TimeDistributed(Flatten(input_shape=base_model.input_shape[1:])))\n",
    "model.add(GRU(256, activation='relu', recurrent_activation='hard_sigmoid', dropout=0.2))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "optimiser = Adam()\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "model_name = 'model_VGGNET_TL' + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "#filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "filepath = model_name + 'model.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n",
    "callbacks_list = [checkpoint, LR]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/133 [==========================>...] - ETA: 6s - loss: 0.2341 - categorical_accuracy: 0.9213"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 88s 665ms/step - loss: 0.2316 - categorical_accuracy: 0.9203 - val_loss: 1.1754 - val_categorical_accuracy: 0.6200\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.62000, saving model to model_VGGNET_TL/model.h5\n",
      "Epoch 2/20\n",
      "123/133 [==========================>...] - ETA: 5s - loss: 0.2095 - categorical_accuracy: 0.9301"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:50: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 85s 641ms/step - loss: 0.2087 - categorical_accuracy: 0.9283 - val_loss: 0.9440 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.62000 to 0.65000, saving model to model_VGGNET_TL/model.h5\n",
      "Epoch 3/20\n",
      "133/133 [==============================] - 85s 641ms/step - loss: 0.1064 - categorical_accuracy: 0.9744 - val_loss: 0.9125 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.65000 to 0.68000, saving model to model_VGGNET_TL/model.h5\n",
      "Epoch 4/20\n",
      "133/133 [==============================] - 85s 641ms/step - loss: 0.0681 - categorical_accuracy: 0.9789 - val_loss: 1.1988 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.68000\n",
      "Epoch 5/20\n",
      "133/133 [==============================] - 85s 641ms/step - loss: 0.0347 - categorical_accuracy: 0.9955 - val_loss: 1.0454 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.68000 to 0.69000, saving model to model_VGGNET_TL/model.h5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 6/20\n",
      "133/133 [==============================] - 85s 642ms/step - loss: 0.0267 - categorical_accuracy: 0.9940 - val_loss: 0.9462 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy improved from 0.69000 to 0.70000, saving model to model_VGGNET_TL/model.h5\n",
      "Epoch 7/20\n",
      "133/133 [==============================] - 85s 641ms/step - loss: 0.0173 - categorical_accuracy: 1.0000 - val_loss: 0.9891 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.70000\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 8/20\n",
      "133/133 [==============================] - 85s 641ms/step - loss: 0.0104 - categorical_accuracy: 1.0000 - val_loss: 0.9493 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.70000\n",
      "Epoch 9/20\n",
      "133/133 [==============================] - 85s 641ms/step - loss: 0.0165 - categorical_accuracy: 0.9975 - val_loss: 1.0088 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy improved from 0.70000 to 0.70000, saving model to model_VGGNET_TL/model.h5\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 10/20\n",
      "133/133 [==============================] - 85s 641ms/step - loss: 0.0087 - categorical_accuracy: 1.0000 - val_loss: 0.9763 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.70000\n",
      "Epoch 11/20\n",
      "133/133 [==============================] - 85s 642ms/step - loss: 0.0090 - categorical_accuracy: 1.0000 - val_loss: 0.9461 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00011: val_categorical_accuracy did not improve from 0.70000\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 12/20\n",
      "133/133 [==============================] - 85s 641ms/step - loss: 0.0070 - categorical_accuracy: 1.0000 - val_loss: 0.9550 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00012: val_categorical_accuracy did not improve from 0.70000\n",
      "Epoch 13/20\n",
      "133/133 [==============================] - 85s 641ms/step - loss: 0.0068 - categorical_accuracy: 1.0000 - val_loss: 0.9621 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00013: val_categorical_accuracy did not improve from 0.70000\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 14/20\n",
      "133/133 [==============================] - 85s 641ms/step - loss: 0.0055 - categorical_accuracy: 1.0000 - val_loss: 0.9632 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00014: val_categorical_accuracy did not improve from 0.70000\n",
      "Epoch 15/20\n",
      "133/133 [==============================] - 85s 641ms/step - loss: 0.0079 - categorical_accuracy: 1.0000 - val_loss: 0.9598 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00015: val_categorical_accuracy did not improve from 0.70000\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 16/20\n",
      "133/133 [==============================] - 85s 641ms/step - loss: 0.0064 - categorical_accuracy: 1.0000 - val_loss: 0.9622 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00016: val_categorical_accuracy did not improve from 0.70000\n",
      "Epoch 17/20\n",
      "133/133 [==============================] - 85s 641ms/step - loss: 0.0086 - categorical_accuracy: 1.0000 - val_loss: 0.9650 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00017: val_categorical_accuracy did not improve from 0.70000\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 18/20\n",
      "133/133 [==============================] - 85s 641ms/step - loss: 0.0083 - categorical_accuracy: 1.0000 - val_loss: 0.9690 - val_categorical_accuracy: 0.6800\n",
      "\n",
      "Epoch 00018: val_categorical_accuracy did not improve from 0.70000\n",
      "Epoch 19/20\n",
      "133/133 [==============================] - 85s 641ms/step - loss: 0.0070 - categorical_accuracy: 1.0000 - val_loss: 0.9700 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00019: val_categorical_accuracy did not improve from 0.70000\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 20/20\n",
      "133/133 [==============================] - 85s 641ms/step - loss: 0.0055 - categorical_accuracy: 1.0000 - val_loss: 0.9688 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00020: val_categorical_accuracy did not improve from 0.70000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f17fc05cbe0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=20, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Model based on transfer learning and RNN :Validation accurancy stuck on 70% while Training accurancy reached to 99.99 %  which means this model is highly overfittting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_17 (TimeDis (None, None, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "time_distributed_18 (TimeDis (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (None, 256)               590592    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 20,616,261\n",
      "Trainable params: 591,877\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## To overcome the problem of overfitting, we need to dropout and batch normalization\n",
    "\n",
    "# To remove overfitting , we will batch normalization and drop out\n",
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(120, 120, 3))\n",
    "\n",
    "# freeze the layers in base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "    # Get the output from the base model \n",
    "base_model_ouput = base_model.output\n",
    "    \n",
    "x = GlobalAveragePooling2D()(base_model_ouput)\n",
    "model = Sequential()\n",
    "base_model = Model(inputs=base_model.input, outputs=x)\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(base_model, input_shape=base_model.input_shape))\n",
    "model.add(TimeDistributed(Flatten(input_shape=base_model.input_shape[1:])))\n",
    "model.add(GRU(256, activation='relu', recurrent_activation='hard_sigmoid', dropout=0.2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "optimiser = Adam()\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "model_name = 'model_VGGNET_TL_1' + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "#filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "filepath = model_name + 'model.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)\n",
    "callbacks_list = [checkpoint, LR]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "109/133 [=======================>......] - ETA: 13s - loss: 1.5924 - categorical_accuracy: 0.3028"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:50: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 86s 649ms/step - loss: 1.5614 - categorical_accuracy: 0.3218 - val_loss: 1.3465 - val_categorical_accuracy: 0.4600\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.46000, saving model to model_VGGNET_TL_1/model.h5\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 83s 626ms/step - loss: 1.3087 - categorical_accuracy: 0.4627 - val_loss: 1.1758 - val_categorical_accuracy: 0.5400\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.46000 to 0.54000, saving model to model_VGGNET_TL_1/model.h5\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 83s 626ms/step - loss: 1.1587 - categorical_accuracy: 0.5208 - val_loss: 1.1492 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.54000 to 0.57000, saving model to model_VGGNET_TL_1/model.h5\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 83s 627ms/step - loss: 1.0419 - categorical_accuracy: 0.5975 - val_loss: 0.9413 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.57000 to 0.60000, saving model to model_VGGNET_TL_1/model.h5\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 84s 628ms/step - loss: 0.9543 - categorical_accuracy: 0.6411 - val_loss: 0.8995 - val_categorical_accuracy: 0.6400\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy improved from 0.60000 to 0.64000, saving model to model_VGGNET_TL_1/model.h5\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 84s 628ms/step - loss: 0.9430 - categorical_accuracy: 0.6301 - val_loss: 0.9503 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.64000\n",
      "Epoch 7/10\n",
      "133/133 [==============================] - 84s 628ms/step - loss: 0.8279 - categorical_accuracy: 0.6862 - val_loss: 0.8588 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy improved from 0.64000 to 0.67000, saving model to model_VGGNET_TL_1/model.h5\n",
      "Epoch 8/10\n",
      "133/133 [==============================] - 84s 629ms/step - loss: 0.8088 - categorical_accuracy: 0.7028 - val_loss: 0.9008 - val_categorical_accuracy: 0.5800\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.67000\n",
      "Epoch 9/10\n",
      "133/133 [==============================] - 84s 629ms/step - loss: 0.7656 - categorical_accuracy: 0.7103 - val_loss: 1.1228 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.67000\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 10/10\n",
      "133/133 [==============================] - 83s 627ms/step - loss: 0.6470 - categorical_accuracy: 0.7519 - val_loss: 0.7496 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy improved from 0.67000 to 0.70000, saving model to model_VGGNET_TL_1/model.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f178aba7ba8>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=10, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99/133 [=====================>........] - ETA: 19s - loss: 0.6153 - categorical_accuracy: 0.7657"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 86s 646ms/step - loss: 0.6181 - categorical_accuracy: 0.7674 - val_loss: 0.7109 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy did not improve from 0.70000\n",
      "Epoch 2/5\n",
      " 97/133 [====================>.........] - ETA: 19s - loss: 0.5639 - categorical_accuracy: 0.7918"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disks/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:50: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 83s 626ms/step - loss: 0.5812 - categorical_accuracy: 0.7744 - val_loss: 0.7657 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy did not improve from 0.70000\n",
      "Epoch 3/5\n",
      "133/133 [==============================] - 83s 628ms/step - loss: 0.5008 - categorical_accuracy: 0.8105 - val_loss: 0.7574 - val_categorical_accuracy: 0.7100\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.70000 to 0.71000, saving model to model_VGGNET_TL_1/model.h5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 4/5\n",
      "133/133 [==============================] - 84s 628ms/step - loss: 0.4557 - categorical_accuracy: 0.8261 - val_loss: 0.6602 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy improved from 0.71000 to 0.73000, saving model to model_VGGNET_TL_1/model.h5\n",
      "Epoch 5/5\n",
      "133/133 [==============================] - 83s 627ms/step - loss: 0.4322 - categorical_accuracy: 0.8501 - val_loss: 0.7301 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.73000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f178aba7cc0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=5, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Model based on transfer learning and RNN :Validation accurancy improved to 73% after normalization and dropouts while Training accurancy = 85% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended Final Model\n",
    "\n",
    "## MODEL 9\n",
    "- Model 9 is our best model. It has provided an accuracy of 83% on the validation set consistently over multiple epochs. Also, it has not overfit the training data (unlike the previous models) with a training accuracy between 87-90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 9 can be downloaded from here:\n",
    "https://drive.google.com/drive/folders/1YDOavo0fBxQXc77tNTevpvRxqtanJeOy?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
